{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d048a4bf",
   "metadata": {},
   "source": [
    "# Playground to get acquainted with LLMs\n",
    "\n",
    "Huggingface playground to manipulate LLMs such as BERT or GPT.\n",
    "\n",
    "Note that fine-tuning a LLM will come next and is not (yet) included into this notebook. The purpose here is to fully illustrate what a LLM is and how we can manipulate it, as a language model on the one hand and as an encoder to yield contextual embeddings on the other hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14042226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM , AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebb624",
   "metadata": {},
   "source": [
    "## Playing with GPT\n",
    "\n",
    "GPT is a causal transformer encoder trained as a language model. The pre-trained model, along with the corresponding tokenizer, can be directly loaded via the Huggingface library simply by specifying a specific checkpoint (aka model name) such as \"gpt2\" or \"bert-base-uncased\". See https://huggingface.co/models for an extensive list of models that can be imported.\n",
    "\n",
    "Note that Huggingface models often come with the transformer encoder along with the associated \"classification head\", depending on the class invoked. For instance, AutoModelForCausalLM will yield the model with a LM classification head that predicts a probability distribution function over the vocabulary from the encoded/contextual representation of the tokens at the input. Simply using AutoModel will yield the encoder with no classification head. \n",
    "\n",
    "We will use both options in this labwork but you have to be aware that there are other options down there such as AutoModelForSequenceClassification (document classification) or AutoModelForTokenClassification (tagging). We will not use these at this stage (nor at a later stage) to clearly evidence what a classification head is and how it works.\n",
    "\n",
    "Huggingface documentation on the GPT2 model: https://huggingface.co/docs/transformers/model_doc/gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7549aaa",
   "metadata": {},
   "source": [
    "### Getting acquainted with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6bfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Loading the transformer encoder as well as the LM classification head that predicts a\n",
    "# probability distribution over the vocabulary from the token contextual embeddings. The\n",
    "# model loaded defines the architecture and the weights, both for the encoder and the LM\n",
    "# classification head.\n",
    "#\n",
    "checkpoint = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) # load tokenizer\n",
    "print(tokenizer)\n",
    "print()\n",
    "\n",
    "#\n",
    "# print a few things about the model -- can you identify some of the important features such\n",
    "# as the embedding dimension, the vocabulary size, the maximum authorized sequence length? No \n",
    "# worries if you don't understand all the parameters, you're not supposed to anyway. \n",
    "# \n",
    "# See https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Config\n",
    "#\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint) # load model\n",
    "print(model.config)\n",
    "\n",
    "print()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47867334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now let's encode a sentence and run it through the model to see what we get out of it\n",
    "#\n",
    "# See https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel for details\n",
    "# on the forward function of a GPT2LMHeadModel model.\n",
    "#\n",
    "\n",
    "text = 'I enjoy playing with LLMs.'\n",
    "\n",
    "#\n",
    "# Let's first have a look at what the tokenizer does.\n",
    "#\n",
    "# Question: Why do you think we have an 'attention_mask' attribute at the output of the tokenizer?\n",
    "#\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print('Output of tokenizer:\\n', inputs)\n",
    "print('Tokens and text:', tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]), '-->', tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "#\n",
    "# Run tokens through the model\n",
    "#\n",
    "# Question: Explain the output that we have \n",
    "#\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print('Output shape:', outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97353093",
   "metadata": {},
   "source": [
    "### Looking at token probabilities and generating language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's have a look at the LM probabilities\n",
    "#\n",
    "\n",
    "#\n",
    "# get all log-probabilities\n",
    "#\n",
    "with torch.no_grad():\n",
    "    logprobs = torch.nn.LogSoftmax(dim=-1)(outputs.logits)[0]\n",
    "\n",
    "#\n",
    "# print LM probabilities for each token in the input\n",
    "#\n",
    "ids = inputs['input_ids'][0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "print('LM probabilities for sequence', tokens)\n",
    "for i in range(len(ids) - 1):\n",
    "    \n",
    "    next_token = ids[i+1].item()\n",
    "    lm_prob = ##### TO COMPLETE ##### --> use .item() to convert tensor to float value\n",
    "    \n",
    "    print('  P[{}|{}] = {:.6f}'.format(tokens[i+1], ' '.join(tokens[:i+1]), lm_prob))\n",
    "    \n",
    "#   \n",
    "# TODO ::: Find most likely token following with in the input\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Write a prompt completion function based on GPT-2, following the idea of the previous labwork.\n",
    "#\n",
    "# For sake of simplicity, we will run the entire sequence through the model at each step rather\n",
    "# than memorizing previous operations to run only one step as we did for RNNs. In other words, \n",
    "# after adding a token to the generated sequence, you will run the entire new sequence through\n",
    "# the model to get the probabilities for the next token. In practice, there are ways to avoid\n",
    "# that so as to be much more efficient.\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873bd690",
   "metadata": {},
   "source": [
    "### Looking at token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We can also get the embeddings of the tokens in addition to the output logits\n",
    "#\n",
    "\n",
    "text = 'I enjoy playing with LLMs.'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_hidden_states = True)\n",
    "\n",
    "print(len(outputs.hidden_states))\n",
    "print(outputs.hidden_states[0].shape, outputs.hidden_states[0][0][0].shape) # initial embeddings + positional encoding\n",
    "print(outputs.hidden_states[-1].shape, outputs.hidden_states[-1][0][0].shape) # last layer's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Here are 15 utterances with the token 'rat' with different meanings and morpho-syntactic functions, plus \n",
    "# one with the token mouse instrad of rat. \n",
    "#\n",
    "# Writea piece of code to visualize the 16 contextual embeddings in 2D with tSNE.\n",
    "#\n",
    "\n",
    "sentences = [\n",
    "\"He decided to rat on his friends to get a lighter sentence.\",\n",
    "\"He's quick to rat out his accomplices.\",\n",
    "\"She felt betrayed when he went to rat her out to the boss.\",\n",
    "\"I can’t believe you would rat on me like that!\",\n",
    "\"The suspect threatened to rat if they didn’t offer a deal.\",\n",
    "\"A rat scurried across the floor last night.\",\n",
    "\"The cat caught a rat in the garden.\",\n",
    "\"The rat is often found in urban areas looking for food.\",\n",
    "\"I heard a rat squeaking in the walls.\",\n",
    "\"The farmer set traps to catch the rat in the barn.\",\n",
    "\"He’s a rat for telling the police everything we did.\",\n",
    "\"No one trusts him anymore because he's known as a rat.\",\n",
    "\"You can’t just rat out your friends like that and expect to be forgiven.\",\n",
    "\"She was labeled a rat after giving up the gang’s hideout.\",\n",
    "\"He tried to act tough, but everyone knew he was a rat who'd sell anyone out.\",\n",
    "\"I heard a mouse squeaking in the walls.\"\n",
    "]\n",
    "\n",
    "#\n",
    "# tokenize all sentences at once: outputs lists rather than tensors to skirt the padding\n",
    "# issue. Will have to convert to tensor before passing along to the model though.\n",
    "#  \n",
    "inputs = tokenizer(sentences) \n",
    "\n",
    "#\n",
    "# retrieve embeddings of the token 'rat' in all sentences\n",
    "#\n",
    "rat_id = tokenizer.encode('a rat is an animal')[1]\n",
    "mouse_id = tokenizer.encode('a mouse is an animal')[1]\n",
    "\n",
    "embeddings = np.empty((len(sentences), model.config.n_embd), dtype='float32')\n",
    "\n",
    "for i in  range(len(sentences)):\n",
    "    token_id = mouse_id if i == len(sentences) - 1 else rat_id\n",
    "    \n",
    "    idx = inputs['input_ids'][i].index(token_id)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor(inputs['input_ids'][i]), output_hidden_states = True)\n",
    "    \n",
    "    embeddings[i,:] = ##### TO COMPLETE ##### --> use .detach().numpy() to get rid of gradients and convert to numpy() array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# tSNE projection with cosine distance\n",
    "#\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "Y = TSNE(n_components=2, metric='cosine', init='random', random_state=0, perplexity=10).fit_transform(embeddings)\n",
    "print(Y.shape)\n",
    "\n",
    "plt.scatter(Y[:,0], Y[:,1])\n",
    "for i in range(len(sentences)):\n",
    "    plt.annotate(str(i+1), xy=(Y[i,0],Y[i,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744891b0",
   "metadata": {},
   "source": [
    "## Playing with BERT\n",
    "\n",
    "BERT is a bidirectional transformer pre-trained with a dual objective: masked language modeling and  next sentence prediction (see course slideware). Similar to GPT2, pre-trained models can be downloaded from the Huggingface library to play with. They might come with a classification head or not, depending on the class used to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb2c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) # load tokenizer\n",
    "print(tokenizer)\n",
    "print()\n",
    "\n",
    "#\n",
    "# print a few things about the model -- can you identify some of the important features such\n",
    "# as the embedding dimension, the vocabulary size, the maximum authorized sequence length? No \n",
    "# worries if you don't understand all the parameters, you're not supposed to anyway. \n",
    "# \n",
    "# See https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Config\n",
    "#\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint) # load model\n",
    "print(model.config)\n",
    "\n",
    "print()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac918fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I enjoy [MASK] with LLMs.'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print('Output of tokenizer:\\n', inputs)\n",
    "print('Tokens and text:', tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]), '--', tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print('Output shape:', outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f970c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# What are the 10 most likely tokens for the mask and the corresponding log-probabilities?\n",
    "#\n",
    "# Hint: you can use the torch.topk() function to get the top k values and indices\n",
    "# of a tensor\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1512cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Take the same 15 sentences with token 'rat' (plus th eone with 'mouse'), get the contextual \n",
    "# embeddings at the output of the BERT model and plot again. The code is roughly the same as\n",
    "# for the gtp2 model\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f84f9",
   "metadata": {},
   "source": [
    "## Final note\n",
    "\n",
    "If you are only interested by the pre-trained encoder, be it a gpt2 encoder or a BERT one, you  can load the models without any classification head, in which case the output directly contains the (contextual) embeddings of the tokens. The following cell for instance illustrate how to do that for a BERT model. In the next lecture, we will use the encoder and the resulting embeddings as part of a neural network architecture and fine-tune the encoder and train the classification elements in the network for a document classification task. \n",
    "\n",
    "See https://huggingface.co/transformers/v3.5.1/model_doc/bert.html#bertmodel for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint, add_pooling_layer=False)\n",
    "print(model.config)\n",
    "\n",
    "print()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I enjoy playing with LLMs.'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print('Output of tokenizer:\\n', inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print('Last hidden states shape:', outputs['last_hidden_state'].shape)\n",
    "\n",
    "\n",
    "# now you can plug these contextual embeddings into a model... but that's for next (and last) session !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4a9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
