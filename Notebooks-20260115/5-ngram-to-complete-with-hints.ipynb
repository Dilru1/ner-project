{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f1b6a9",
   "metadata": {},
   "source": [
    "# Notebook illustrating the lecture on LMs\n",
    "\n",
    "We will use the NLK module LM to better understand what LMs are and how they work. \n",
    "\n",
    "In the first part of the notebook, we will guide you through this module to get a better understanding and illustrate the notions that wer've seen during the lecture. You'll get to see counts, estimate LMs from the counts with a number of dicounting techniques and measure log-prob/perplexity of a text.\n",
    "\n",
    "In the second part, you will be guided through training a small-sized LM on real data, from different amount of data. With a trained LM, you're then asked to write a function that generates text by sampling from the LM distribution.\n",
    "\n",
    "Here are useful reference links to the documentation of the LM module of NLTK and of the main classes and functions that we will make use of.\n",
    "\n",
    "Entry points in the documentation:\n",
    "- https://www.nltk.org/api/nltk.lm.html\n",
    "- https://www.nltk.org/howto/lm.html\n",
    "\n",
    "You will be given more targeted links in Part I of the notebook.\n",
    "\n",
    "Also remember to have a look at Chen and Goodman's paper if you want to have details on the various discounting, interpolation and backoff techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7efc162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "\n",
    "from nltk.lm import NgramCounter, Vocabulary\n",
    "from nltk.lm.models import MLE, Laplace, WittenBellInterpolated, KneserNeyInterpolated, AbsoluteDiscountingInterpolated\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.util import ngrams, everygrams\n",
    "\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b546dc",
   "metadata": {},
   "source": [
    "## Part I -- Playing with a toy example to understand how things work\n",
    "\n",
    "Let's play with a very small artificial corpus to get acquainted with NLTK's LM module and verify it does exactly what we've seen in the classroom. The idea is to\n",
    "1. see the interplay between vocabulary and LM with cutoff and unk\n",
    "2. get ngram probabilities with MLE estimation and Laplace smoothing\n",
    "3. compute the probability / perplexity of a sequence\n",
    "\n",
    "Let's first define a toy corpus of texts to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [['<s>', 'a', 'b', 'a', '</s>'], \n",
    "        ['<s>', 'a', 'c', 'b', 'a', 'b', '</s>'], \n",
    "        ['<s>', 'a', 'a', 'c', 'b', '</s>']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1401bd5",
   "metadata": {},
   "source": [
    "### Define the vocabulary\n",
    "\n",
    "First thing to do is to define the vocabulary on which the language model operates. \n",
    "\n",
    "The vocabulary can be conveniently obtained from the (flattened) text corpus, using a specified cutoff, say n, to retain only tokens that appear at least n times in the data.\n",
    "\n",
    "See https://www.nltk.org/api/nltk.lm.vocabulary.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec86ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flat_text = [token for utterance in text for token in utterance]\n",
    "\n",
    "vocab = Vocabulary(flat_text, unk_cutoff=1) # define the vocabulary\n",
    "print('vocab size =', len(vocab))\n",
    "print('vocab =', vocab.counts)\n",
    "\n",
    "# Note for later use that the vocab object has a lookup method that maps tokens in \n",
    "# an input utterance, e.g.,\n",
    "test = ('<s>', 'a', 'b', 'a', 'b', 'c', '</s>')\n",
    "print()\n",
    "print('Encoding', test, 'as', vocab.lookup(test))\n",
    "\n",
    "# Let's try with cutoff=3: Note that you don't need to recompute the vocabulary \n",
    "# counts and can make use of the counts computed in vocab to get a new vocab \n",
    "# with different counts\n",
    "\n",
    "vocab2 = Vocabulary(vocab.counts, unk_cutoff=3)\n",
    "print('Re encoding', test, 'as', vocab2.lookup(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fd075",
   "metadata": {},
   "source": [
    "### Creating and counting ngrams\n",
    "\n",
    "NLTK provides a counter of ngrams, `NgramCounter()`, that can be used to compute, store and manipulate the counts of each ngrams that appear in a corpus.\n",
    "\n",
    "Unfortunately, NgramCounter() does not take text as input but rather the list of ngrams to be considered in a text. In other words, for the text `['<s>', 'a', 'b', '</s>'` and considering a maximum ngram order of 2, updating the counter requires to input `('<s>',), ('a',), ('b',), ('</s>',), ('<s>', 'a'), ('a', 'b'), ('b', '</s>')`.\n",
    "\n",
    "Fotunately, NLTK provides the function `everygram()` that converts a text into the corresponding list of ngrams. Note that there are other similar functions (that we will not use in this notebook) such as `ngram()`, `bigram()`, `trigram()`.\n",
    "\n",
    "In the cells below, with illustrate things with a LM order of 3, i.e., we go up to trigrams but no further. Hence we only generate unigrams, bigrams and trigrams in the counts and estimate the LM as a trigram. You can easily change the order.\n",
    "\n",
    "See https://www.nltk.org/api/nltk.lm.counter.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a89226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrating everygram() before we use it:\n",
    "print(text[0], '-->', list(everygrams(text[0], max_len=3)), '\\n')\n",
    "\n",
    "counts = NgramCounter([everygrams(s, max_len=3) for s in text])\n",
    "print('Total number of ngrams =', counts.N())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7381f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access counts by specifying the history (called context in NLTK's terminology) \n",
    "# as an index: this provides all the possible completion for this history and the number \n",
    "# of times you've seen this completion. Note that the history is a list (not a tuple).\n",
    "\n",
    "print(\"After 'a':\", sorted(counts[['a']].items())) # get bigrams starting with 'a'\n",
    "print(\"After 'c':\", sorted(counts[['c']].items())) \n",
    "\n",
    "print(\"After 'a', 'b':\", sorted(counts[['a', 'b']].items())) # get trigrams starting with ('a', 'b')\n",
    "print(\"After 'c', 'b':\", sorted(counts[['c', 'b']].items()))\n",
    "print()\n",
    "\n",
    "# You can also access all counts for a given order\n",
    "print('Accessing ngrams for a given order directly:')\n",
    "unigram = counts[1] # a dictionary token=count\n",
    "print(list(unigram.keys()))\n",
    "print('unigram counts:', sorted(unigram.items()))\n",
    "\n",
    "bigram = counts[2] # a dictionary history=dictionary of counts for the bigram following this history\n",
    "print(list(bigram.keys()))\n",
    "print(\"bigram counts starting with 'a':\", sorted(bigram[('a',)].items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d698e",
   "metadata": {},
   "source": [
    "### Estimating LM probabilities\n",
    "\n",
    "From the counts and vocabulary, one can easily estimate LM probabilities and instantiate a LM to get the probabilities of a sequence or simply that of a particular ngram.\n",
    "\n",
    "Let's try with a maximum likelhood estimation of the ngram probabilities and then let's try smoothing.\n",
    "\n",
    "See https://www.nltk.org/api/nltk.lm.api.html for the general definition of the LM classe and https://www.nltk.org/api/nltk.lm.models.html for specific models that may include discounting and (recursive) interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce351f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MLE LM')\n",
    "\n",
    "# Create and estimate LM from the vocabulary and counts\n",
    "lm = MLE(3, vocabulary=vocab, counter=counts)\n",
    "\n",
    "print('  P[b|h=a] = {:.4f}'.format(lm.score('b', context=('a',))))\n",
    "print('  P[b|h=c] = {:.4f}'.format(lm.score('b', context=('c',))))\n",
    "print('  P[c|h=ab] = {:.4f}'.format(lm.score('c', context=('a','b'))))\n",
    "print('  P[a|h=cb] = {:.4f}'.format(lm.score('a', context=('c','b'))))\n",
    "\n",
    "# You also have a logscore() function if you want the log-probability (base 2) directly\n",
    "\n",
    "# TODO: Verify that these probabilities are the ones you expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Laplace smoothing')\n",
    "\n",
    "lm = Laplace(3, vocabulary=vocab, counter=counts)\n",
    "\n",
    "print('  P[b|h=a] = {:.4f}'.format(lm.score('b', context=('a',))))\n",
    "print('  P[b|h=c] = {:.4f}'.format(lm.score('b', context=('c',))))\n",
    "print('  P[c|h=ab] = {:.4f}'.format(lm.score('c', context=('a','b'))))\n",
    "print('  P[a|h=cb] = {:.4f}'.format(lm.score('a', context=('c','b'))))\n",
    "\n",
    "# TODO: Verify that these probabilities are the ones you expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49202cae",
   "metadata": {},
   "source": [
    "And there are many other discounting and smoothing schemes implemented in NLTK LM module, among which:\n",
    "- Lindstone: same as Laplace except you specify the value to add (Lindstone with 1 == Laplace)\n",
    "- AbsoluteDiscountingInterpolated: asbolute discounting with interpolation as in the lecture\n",
    "- KneserNeyInterpolated: a smart variant of absolute discounting\n",
    "- WittenBellInterpolated: a variant of Jelinek-Mercer smoothing implementing recursive interpolation, no discounting\n",
    "\n",
    "Invoking such LMs is straightforward from the above examples. Note however that some discounting schemes have specific parameters that you can specify. For instance, absolute discounting requires the fix the discount value. This is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55486782",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Absolute Discounting with a dicount of 0.2')\n",
    "\n",
    "lm = AbsoluteDiscountingInterpolated(3, discount=0.2, vocabulary=vocab, counter=counts)\n",
    "print('  P[b|h=a] = {:.4f}'.format(lm.score('b', context=('a',))))\n",
    "print('  P[b|h=c] = {:.4f}'.format(lm.score('b', context=('c',))))\n",
    "print('  P[c|h=ab] = {:.4f}'.format(lm.score('c', context=('a','b'))))\n",
    "print('  P[a|h=cb] = {:.4f}'.format(lm.score('a', context=('c','b'))))\n",
    "\n",
    "print('\\nAbsolute Discounting with a dicount of 0.7')\n",
    "lm = AbsoluteDiscountingInterpolated(3, discount=0.7, vocabulary=vocab, counter=counts)\n",
    "print('  P[b|h=a] = {:.4f}'.format(lm.score('b', context=('a',))))\n",
    "print('  P[b|h=c] = {:.4f}'.format(lm.score('b', context=('c',))))\n",
    "print('  P[c|h=ab] = {:.4f}'.format(lm.score('c', context=('a','b'))))\n",
    "print('  P[a|h=cb] = {:.4f}'.format(lm.score('a', context=('c','b'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7ff38",
   "metadata": {},
   "source": [
    "**We're now all set to move to real data and have fun!**\n",
    "\n",
    "A few things that might be useful to know before moving on.\n",
    "\n",
    "- Remember the `'<UNK>'` token that appeared when we encoded with a vocabulary built with a cutoff at three occurrences? We didn't really pay attention to this so far because we played with a closed LM where no unknown tokens appear in the training and test data. In practice, we need to encode with `vocab.lookup()` the data (train and test) before generating the ngrams, counts and probabilities. Note that the `lm.score()` and `lm.logscore()` function does the `vocab.lookup()` operation. But `everygram()` does not so you'll need to do it by yourself.  \n",
    "\n",
    "- We will have to compute perplexity of texts to evaluate the various LMs we will build. NLTK provides a perplexity function that you can make use of. But I'm not super happy with how it's implemented and sort of disagree with how perplexity is computed (better said, there are high risks that you do the wrong thing if you use it directly). We will thus reimplement one.\n",
    "\n",
    "- Note that `Vocabulary()` and `NgramCounter()` both provide an `update()` method so you don't really have to provide all the required information at once: you can invoke `update()` in an iterative manner while you scan your data. This is not super efficient though and we fortunately don't need that here. Just in case you need that at some point. \n",
    "\n",
    "Anyway, there's much more than what's illustrated here with this library. But keep in mind that it's by far nt the most efficient and practical implementation of ngram LMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd9cb4",
   "metadata": {},
   "source": [
    "## Part II -- Play with real data and make real LMs\n",
    "\n",
    "To faciliate things, we provide you with pre-processed data from the Signal Media 1M article dataset. It consists of a fairly large number of news articles (from a decade back). Details on the dataset can be found at https://ceur-ws.org/Vol-1568/paper8.pdf and the original data can be downloaded from https://research.signal-ai.com/newsir16/signal-dataset.html.\n",
    "\n",
    "To make things easy, the original corpus went through heavy article and utterance selection and cleansing, preprocessing with spaCy's en_web_core_md pipeline. We kept the first 100k articles, with the following global statistics:\n",
    "```\n",
    "# of docs = 100,000 (out of 886,145 in the full dataset after basic filtering)\n",
    "avg. # of utterances per doc = 3.28897\n",
    "total number of utterances = 328897\n",
    "stats on utterance length: mean=26.90  median=24  stdev=19.469142738145738 min=1  max=2158\n",
    "```\n",
    "These 100k documents were split into 90k documents for training, 5k for validation, 5k for test. For each fold, we made a list of sentences with a number of tokens between 10 and 100, resulting in 271,607 utterances for training, 15,316 for validation and 15,062 for testing.\n",
    "\n",
    "You can get this dataset from the json.gzip file provided which has the following format:\n",
    "```\n",
    "data['train'] # list of utterances in train\n",
    "  data['train'][0] # list of tokens\n",
    "  data['train'][1]\n",
    "  ...\n",
    "data['valid']\n",
    "  ...\n",
    "data['test']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ecba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271607 15316 15062\n",
      "[['<s>', 'Ally', 'Financial', 'Inc.', ' ', 'is', 'a', 'leading', 'automotive', 'financial', 'services', 'company', 'powered', 'by', 'a', 'top', 'direct', 'banking', 'franchise', '</s>'], ['<s>', 'Ally', \"'s\", 'automotive', 'services', 'business', 'offers', 'a', 'full', 'spectrum', 'of', 'financial', 'products', 'and', 'services', ',', 'including', 'new', 'and', 'used', 'vehicle', 'inventory', 'and', 'consumer', 'financing', ',', 'leasing', ',', 'vehicle', 'service', 'contracts', ',', 'commercial', 'loans', 'and', 'vehicle', 'remarketing', 'services', ',', 'as', 'well', 'as', 'a', 'variety', 'of', 'insurance', 'offerings', ',', 'including', 'inventory', 'insurance', ',', 'insurance', 'consultative', 'services', 'for', 'dealers', 'and', 'other', 'ancillary', 'products', '</s>'], ['<s>', 'Ally', 'Bank', ',', 'the', 'company', \"'s\", 'direct', 'banking', 'subsidiary', 'and', 'member', 'FDIC', ',', 'offers', 'an', 'array', 'of', 'deposit', 'products', ',', 'including', 'certificates', 'of', 'deposit', ',', 'savings', 'accounts', ',', 'money', 'market', 'accounts', ',', 'IRA', 'deposit', 'products', 'and', 'interest', 'checking', '</s>'], ['<s>', 'Ally', \"'s\", 'Corporate', 'Finance', 'unit', 'provides', 'financing', 'to', 'middle', '-', 'market', 'companies', 'across', 'a', 'broad', 'range', 'of', 'industries', '</s>'], ['<s>', 'Established', 'in', '1958', ',', 'Ritchie', 'Bros.', ' ', 'is', 'the', 'world', \"'s\", 'largest', 'seller', 'of', 'used', 'equipment', 'for', 'the', 'construction', ',', 'transportation', ',', 'agriculture', ',', 'material', 'handling', ',', 'energy', ',', 'mining', ',', 'forestry', ',', 'marine', 'and', 'other', 'industries', '</s>'], ['<s>', 'Ritchie', 'Bros.', 'TM', 'solutions', 'make', 'it', 'easy', 'for', 'the', 'world', \"'s\", 'builders', 'to', 'buy', 'and', 'sell', 'equipment', 'with', 'confidence', ',', 'including', 'live', 'unreserved', 'public', 'auctions', 'with', 'on', '-', 'site', 'and', 'online', 'bidding', ',', 'the', 'EquipmentOneTM', 'secure', 'online', 'marketplace', ',', 'a', 'professional', 'corporate', 'asset', 'management', 'program', ',', 'and', 'a', 'range', 'of', 'value', '-', 'added', 'services', ',', 'including', 'equipment', 'financing', 'for', 'customers', 'through', 'Ritchie', 'Bros.', 'Financial', 'Services', '</s>'], ['<s>', 'Ritchie', 'Bros.', 'has', 'operations', 'in', '19', 'countries', ',', 'including', '44', 'auction', 'sites', 'worldwide', '</s>'], ['<s>', 'The', 'sky', 'is', 'the', 'limit', 'when', 'it', 'comes', 'to', 'crafting', 'materials', '</s>'], ['<s>', 'What', 'you', 'include', 'really', 'depends', 'on', 'your', 'child', \"'s\", 'age', 'and', 'interests', '</s>'], ['<s>', 'Do', \"n't\", 'worry', 'if', 'you', 'do', \"n't\", 'have', 'much', 'of', 'a', 'budget', 'for', 'this', 'project', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "with gzip.open('./signalmedia-utterances.json.gz', 'rt', encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(len(data['train']), len(data['valid']), len(data['test']))\n",
    "print(data['train'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b50f70",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Let's first have a look at the vocabulary: \n",
    "\n",
    "- we'll use the flatten() utility function provided by NLTK which makes a (lazy) flat list of all the tokens in all utterances in the train data, efficient equivalent of  `[token for utterance in data['train'] for token in utterance]`\n",
    "- we'll use no unk_cutoff to keep all counts initially, playing with cutoff in a second stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3ebbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(flatten(data['train']), unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c3a046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff= 1   vocsize=159561    %oov=0.00/1.07\n",
      "cutoff= 2   vocsize= 88038    %oov=0.91/1.62\n",
      "cutoff= 3   vocsize= 65258    %oov=1.49/2.06\n",
      "cutoff= 4   vocsize= 53831    %oov=1.93/2.44\n",
      "cutoff= 5   vocsize= 46327    %oov=2.31/2.77\n",
      "cutoff= 6   vocsize= 41202    %oov=2.64/3.07\n",
      "cutoff= 7   vocsize= 37156    %oov=2.94/3.35\n",
      "cutoff= 8   vocsize= 34041    %oov=3.22/3.62\n",
      "cutoff= 9   vocsize= 31447    %oov=3.49/3.89\n",
      "cutoff=10   vocsize= 29375    %oov=3.72/4.10\n",
      "cutoff=11   vocsize= 27552    %oov=3.96/4.31\n",
      "cutoff=12   vocsize= 26021    %oov=4.17/4.51\n",
      "cutoff=13   vocsize= 24664    %oov=4.38/4.69\n",
      "cutoff=14   vocsize= 23473    %oov=4.57/4.88\n",
      "cutoff=15   vocsize= 22408    %oov=4.76/5.05\n",
      "cutoff=16   vocsize= 21462    %oov=4.94/5.20\n",
      "cutoff=17   vocsize= 20623    %oov=5.12/5.34\n",
      "cutoff=18   vocsize= 19917    %oov=5.27/5.49\n",
      "cutoff=19   vocsize= 19208    %oov=5.43/5.65\n"
     ]
    }
   ],
   "source": [
    "# Let's see what happens when we use a cut_off to select the vocabulary and how the out-of-vocabulary (OOV) rate\n",
    "# evolves as we limit the vocabulary size.\n",
    "#\n",
    "# I'm afraid this takes a little bit of time: if too slow or painful, you can compute the OOV rate on a fraction\n",
    "# only of the train and test set to get an idea.\n",
    "\n",
    "def oov_rate(_data, _vocab) -> float:\n",
    "    \"\"\"\n",
    "    Returns the out of vocabulary rate in the data.\n",
    "    \n",
    "    _dat: list of utterances, not encoded with lookup()\n",
    "    _vocab: vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    nunk, ntot = 0, 0\n",
    "    \n",
    "    for u in _data:\n",
    "        # nunk += sum(map(lambda x: x == '<UNK>', _vocab.lookup(u))) # select '<unk>' tokens in the list and\n",
    "        nunk += sum(map(lambda x: x == '<UNK>', _vocab.lookup(u)))\n",
    "        ntot += len(u)\n",
    "    \n",
    "    return 100 * nunk / ntot\n",
    "\n",
    "\n",
    "for cutoff in range(1,20):\n",
    "    V = Vocabulary(vocab.counts, unk_cutoff=cutoff)\n",
    "    oov1, oov2 = oov_rate(data['train'], V), oov_rate(data['test'], V)\n",
    "    print('cutoff={:-2d}   vocsize={:6d}    %oov={:.2f}/{:.2f}'.format(cutoff, len(V), oov1, oov2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316cf266",
   "metadata": {},
   "source": [
    "### Counts\n",
    "\n",
    "Fix the vocabulary and generate the counts. We will generate counts for different corpus size and see how counts evolve. Obviously, in the end, we'll take the counts from the whole corpus.\n",
    "\n",
    "\n",
    "**TODO** Make a (wise) choice for the unk_cutoff value to define your vocabulary before moving on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e170de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# It should take 1 or 2 minutes to compute all the counts, be patient!\n",
    "\n",
    "cutoff = 18\n",
    "V = Vocabulary(vocab.counts, unk_cutoff=cutoff)\n",
    "\n",
    "counts = NgramCounter()\n",
    "counts.update([everygrams(V.lookup(s), max_len=3) for s in data['train']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6b68a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#utterances = 271607\n",
      "   order=0   #distinct=   19917   #total=7855042\n",
      "   order=1   #distinct= 1219589   #total=7583435\n",
      "   order=2   #distinct= 3636446   #total=7311828\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's have a look at the counts\n",
    "#\n",
    "\n",
    "def count_sum(_counts, order=3):\n",
    "    \"\"\"\n",
    "    Sum counts to return the number of unique ngrams (n \\in [1,order]) and the number of occurrences\n",
    "    \"\"\"\n",
    "\n",
    "    stats = []\n",
    "    \n",
    "    ngram = _counts[1]\n",
    "    stats.append([len(ngram), sum(ngram.values())])\n",
    "    \n",
    "    for order in range(1,order): # foreach history/context, we have a dictionary of completion/n key/values        \n",
    "        \n",
    "        ndistincts, noccurrences = 0, 0\n",
    "        \n",
    "        for v in _counts[order+1].values():\n",
    "            ndistincts += len(v)\n",
    "            noccurrences += sum(v.values())\n",
    "\n",
    "        stats.append([ndistincts, noccurrences])\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print('#utterances =', len(data['train']))\n",
    "ntot = count_sum(counts, order=3)\n",
    "for i in range(3):\n",
    "    print('   order={}   #distinct={:8d}   #total={}'.format(i, ntot[i][0], ntot[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Just to illustrate the tables seen in the classroom, let's compute these stats \n",
    "# (number of distinct ngrams, number of ngram occurrences\n",
    "# \n",
    "\n",
    "for n in (50000, 100000, 200000):\n",
    "    print('#utterances =', n)\n",
    "    \n",
    "    c = NgramCounter()\n",
    "    c.update([everygrams(V.lookup(s), max_len=3) for s in data['train'][:n]])\n",
    "\n",
    "    ntot = count_sum(c, order=3)\n",
    "    for i in range(3):\n",
    "        print('   order={}   #distinct={:8d}   #total={}'.format(i, ntot[i][0], ntot[i][1]))\n",
    "    # print('   #total={}'.format(sum([x[1] for x in ntot])))\n",
    "\n",
    "# QUESTION: What do you observe in these numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e7528",
   "metadata": {},
   "source": [
    "### Building and comparing models\n",
    "\n",
    "Now ready to create a first model and evaluate it.\n",
    "\n",
    "A quick recall on evaluation through perplexity. Perplexity is defined as\n",
    "$$\n",
    "P(C) = 2^{-\\mbox{avg_log_prob}(C)}\n",
    "$$\n",
    "where $\\mbox{avg_log_prob}(C)$ is the average ngram log-probability (base 2) measured on the corpus $C$, i.e.,\n",
    "$$\n",
    "    \\mbox{avg_log_prob}(C) = \\sum_{hw \\in \\mbox{ngrams}(C)} \\log_2 P[w|h] \\enspace .\n",
    "$$\n",
    "Here, $\\mbox{ngrams}(C)$ designates all occurrences of ngrams hw (history followed by w) that occur in the corpus $C$. The ngram occurrences can easily be obtained through `ngrams(utterance)` after vocabulary lookup (to map unknown tokens to `'<UNK>'`) for all utterances in $C$.\n",
    "\n",
    "We provide a quick and dirty implementation of perplexity hereunder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81bab0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_logscore(_data, _vocab, _lm, order=None) -> float:\n",
    "    \"\"\"\n",
    "    Return the average ngram log-prob of the utterances in _data\n",
    "    \n",
    "    Input:\n",
    "      _data: input utterances on which to measure ngram log-probs\n",
    "      _vocab: vocabulary (to map unknown tokens)\n",
    "      _lm: language model\n",
    "      \n",
    "    Options:\n",
    "      order=n use ngrams (defaults to LM order)\n",
    "      \n",
    "    Notes: This is a dirty implementation that distegard the first m-grams \n",
    "    for m < n of the utterance but that'll do the job.\n",
    "    \"\"\"\n",
    "    \n",
    "    order = _lm.order if order == None else order\n",
    "    \n",
    "    buf = [ngrams(_vocab.lookup(x), order) for x in _data]\n",
    "    \n",
    "    return mean([_lm.logscore(x[-1], x[:-1]) for x in flatten(buf)])\n",
    "\n",
    "\n",
    "def perplexity(_data, _vocab, _lm, order=None) -> float:\n",
    "    \"\"\"\n",
    "    Return the perplexity measured on the utterances in _data\n",
    "    \n",
    "    Input:\n",
    "      _data: input utterances on which to measure ngram log-probs\n",
    "      _vocab: vocabulary (to map unknown tokens)\n",
    "      _lm: language model\n",
    "      \n",
    "    Options:\n",
    "      order=n use ngrams (defaults to LM order)    \n",
    "    \"\"\"\n",
    "    \n",
    "    return pow(2.0, -avg_logscore(_data, _vocab, _lm, order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82661ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some of the models are pretty slow, we make a small version of the test utterances to measure perplexity\n",
    "\n",
    "C = data['test'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344e8b6",
   "metadata": {},
   "source": [
    "**At this stage, you should be able to play on your own!**\n",
    "\n",
    "So here's what you're asked to do:\n",
    "1. Measure (on the downsized test data) the perplexity of a trigram LM (a) with no smoothing and (b) with Laplace smoothing. How do you explain the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "137f8441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "4224.250426340526\n"
     ]
    }
   ],
   "source": [
    "lm = MLE(3, vocabulary=V, counter=counts)\n",
    "print(perplexity(C, V, lm))\n",
    "\n",
    "lm = Laplace(3, vocabulary=V, counter=counts)\n",
    "print(perplexity(C, V, lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5015fec",
   "metadata": {},
   "source": [
    "2. Using absolute discounting, compare the perplexities of the bigram and trigrem LMs with different discount factors, filling in the table below. What is the impact of the discount factor on the two models?\n",
    "\n",
    "| discount | 0.4 | 0.6 | 0.8 | 1 |\n",
    "|----------|-----|-----|-----|---|\n",
    "| bigram   |     |     |     |   |\n",
    "| trigram  |     |     |     |   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25f6a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 216.57293899633783 142.5137685878291\n",
      "0.6 212.15880849925716 127.18462408534691\n",
      "0.8 212.3927900785234 121.98059526438729\n",
      "1 232.6537718597303 155.7681837866273\n"
     ]
    }
   ],
   "source": [
    "for d in (0.4, 0.6, 0.8, 1):\n",
    "    lm = AbsoluteDiscountingInterpolated(3, discount=d, vocabulary=V, counter=counts)\n",
    "    \n",
    "    print(d, perplexity(C, V, lm, order=2), perplexity(C, V, lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50266f3f",
   "metadata": {},
   "source": [
    "3. Retrain a model with absolute discounting on a smaller train set, say 50k utterances rather than the full 271k ones, and compare with what you had previously. Is the optimal discount factor the same and why? How's perplexity affected by the training dataset size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e48aca",
   "metadata": {},
   "source": [
    "4. Check with Witten-Bell\n",
    "\n",
    "*Corpus is too small for Kneser-Ney so you'll probably get an error if we try that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f51a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WittenBellInterpolated(3, vocabulary=V, counter=counts)\n",
    "print(perplexity(C, V, lm, order=2), perplexity(C, V, lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b646b18",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "You should now have enough experience to write your own text generator given a LM. The idea is to write a function that takes on a prompt and complete it given a LM. To simplify things, we will assume your prompt to be tokenized with a tokenization that resembles the one used to prepare the data.\n",
    "\n",
    "*Hint*: Given an history as a tuple, you can easily get the probability distribution over the vocabulary with\n",
    "```\n",
    "h = ('certificates', 'of')\n",
    "distrib = [lm.score(w, h) for w in vocab]\n",
    "```\n",
    "and use `numpy.random.choice()` to pick one word from this distribution.\n",
    "\n",
    "Unfortunately, with NLTK, this is a bit long to compute with a 20k vocabulary if you use an interpolated LM (which I strongly recommand) but it remains manageable. You may want to downsize your vocabulary of not patient enough.\n",
    "\n",
    "Once you're done with your generation function, enjoy and play with it, try with various smoothing techniques or with various LM order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff641e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(_lm, _vocab, prompt=['<s>'], order=None, maxlen=100) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generate text from the prompt based for the given LM.\n",
    "    \n",
    "    Input:\n",
    "      _lm: language model\n",
    "      _vocab: vocabulary\n",
    "      \n",
    "    Options:\n",
    "      prompt=   prompt as a list[str]\n",
    "      order=    order of the LM (defaults to LM order)\n",
    "      maxlen=   stop generating after maxlen tokens\n",
    "    \"\"\"\n",
    "    text = [token for token in prompt]\n",
    "    \n",
    "    # TO BE COMPLETED\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
