{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940b3679",
   "metadata": {},
   "source": [
    "## Installation instructions\n",
    "\n",
    "Hands-on practice will rely on a (fairly large) number of python packages for NLP and deep learning. If you are running on Google colaboratory, most of these packages are installed in the default environment and there's not much to do. If you want to run on your machine, you will have to install a number of things following the instructions below. Should be fairly straightforward but requires a bit of disk space.\n",
    "\n",
    "Open a shell window and follow the commands below (not sure exactly how to do that on Windows).\n",
    "\n",
    "### Step 1. Set up a virtual environment (optional but recommanded)\n",
    "\n",
    "> export LANG=en_US.UTF-8\n",
    "> python -m venv ./nlp.env\n",
    "> source ./nlp.env.bin.activate\n",
    "\n",
    "If you want to deactivate the virtual environment at some point, simply type deactivate.\n",
    "\n",
    "\n",
    "### Step 2. Add your virtual environment as a kernel in jupyter\n",
    "\n",
    "> pip install ipykernel\n",
    "> python -m ipykernel install --user --name=NLP\n",
    "\n",
    "You can now run your jupyter notebook where you'll see a NLP kernel. But it's empy as of now so we need to install a bunch of things first. \n",
    "\n",
    "### Step 3. Install the necessary packages\n",
    "\n",
    "From your favorite shell, virtual environment activated, you can add the followng package (might take some time):\n",
    "\n",
    "> pip install ntlk\n",
    "> pip install spacy\n",
    "> pip install transformers\n",
    "> pip install scikit-learn\n",
    "> pip install matplotlib\n",
    "\n",
    "You need to install spaCy's processing pipeline resources also. This also holds for colaboratory where you will have to run the following command in the first cell and reload the kernel.\n",
    "\n",
    "> python -m spacy download en_core_web_md\n",
    "\n",
    "You also need to install a number of NLTK resources that we will use. No need to restart the kernel.\n",
    "\n",
    "> python -m nltk.downloader wordnet\n",
    "> python -m nltk.downloader sentiwordnet\n",
    "> python -m nltk.downloader averaged_perceptron_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991ca6a",
   "metadata": {},
   "source": [
    "## Play with tokenization with different toolkits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with NLTK, one of the most popular toolkit for NLP. \n",
    "# See https://www.nltk.org/ for details.\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c589f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's play a bit with the notion of tokenization\n",
    "#\n",
    "\n",
    "# a basic tokenization of a text\n",
    "s='A $2 example\\nof a sentence. And a 2nd sentence.'\n",
    "print([token for token in word_tokenize(s)])\n",
    "\n",
    "# Now combining sentence tokenization with word tokenization\n",
    "print([word_tokenize(x) for x in sent_tokenize(s)])\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "#\n",
    "# You're turn to play a bit and try to fool the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with spaCy now, another popular toolkit that comes with full processing\n",
    "# pipelines that do a lot of things. Tokenization is one of the thing spaCy does but\n",
    "# pipelines do a lot more than that. We'll get back to this later on, only looking \n",
    "# into the tokenization step as of now. See https://spacy.io/ for details.\n",
    "\n",
    "# Load spaCy's English NLP pipeline\n",
    "import spacy\n",
    "process = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's play again with the tokenizer embedded in the processing pipeline that spaCy implements (more details later)\n",
    "#\n",
    "\n",
    "print([token for token in process(s)])\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "#\n",
    "# You're turn to play a bit and see how it behaves on the examples you played with previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f94cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a third one that is currently being used with deep transformer models.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(s)\n",
    "print(tokens)\n",
    "\n",
    "tokens = tokenizer.tokenize(\"But transformers require a limited number of tokens to be efficient.\")\n",
    "print(tokens)\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "#\n",
    "# What do you observe on the tokenization of the last sentence? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0312de",
   "metadata": {},
   "source": [
    "### Lexical analysis: mophology and morphosyntax\n",
    "\n",
    "In the following cells, we will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f71b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NLTK has a number of tools to perform stemming, lemmatization and POS tagging to play with\n",
    "#\n",
    "# We start by importing the tools we want to play with in the next cell\n",
    "# \n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7bd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Examples of a basic stemmer and lemmatizer that takes isolated wordforms and\n",
    "# convert them to the corresponding stem/lemma.\n",
    "#\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"{:10}   {:10}   {:10}\".format('word', 'stem', 'lemma'))\n",
    "print(\"----------------------------------\")\n",
    "for w in ('candy', 'candies', 'loves', 'loved', 'lovely', 'argued', 'arguing'):\n",
    "    print(\"{:10}   {:10}   {:10}\".format(w, stemmer.stem(w), lemmatizer.lemmatize(w)))\n",
    "\n",
    "# Now, what if we pass on a POS information to the lemmatizer?\n",
    "print(\"\\nlemma of the verb 'argued':\", lemmatizer.lemmatize('argued', pos=wordnet.VERB))\n",
    "print(\"lemma of the verb 'arguing':\", lemmatizer.lemmatize('arguing', pos=wordnet.VERB))\n",
    "\n",
    "# Examples of a POS tagger\n",
    "\n",
    "# POS tagging\n",
    "s = 'The cat sat on the mat and had a couple of fishes for dinner.'\n",
    "tokens = word_tokenize(s)\n",
    "tags = pos_tag(tokens)\n",
    "print('\\nSentence:', s)\n",
    "print('Tokens:', tokens)\n",
    "print('POS Tag', tags) # if we want only tags: [x[1] for x in tags]\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Your turn to play a bit with these three steps and try to fool them. \n",
    "# What does POS tagging do with semantically ambiguous sentences?\n",
    "\n",
    "\n",
    "# GOING FURTHER\n",
    "#\n",
    "# One can combine NLTK's POS tagger and WordNetLemmatizer to create a function that converts\n",
    "# an input sentence to a list of tokens with their corresponding POS tags and lemmas. However\n",
    "# not totally straightforward.\n",
    "#\n",
    "# See https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5321a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We can do it all at once with spaCy's pipeline that does tokenization and \n",
    "# POS tagging and lemmatization (and a few other things)\n",
    "#\n",
    "\n",
    "res = process(s)            # -> apply pipeline on text\n",
    "\n",
    "for token in res:           # -> iterate over tokens in doc\n",
    "  print('{:8} {:6} {:4} {}'.format(token.text, token.pos_, token.tag_, token.lemma_))\n",
    "\n",
    "#\n",
    "# You can also do it with long texts and multiple sentences\n",
    "#\n",
    "s = 'The cat sat on the mat. He had a couple of fishes for dinner.'\n",
    "res = process(s)\n",
    "\n",
    "for sent in res.sents:\n",
    "    print('\\nSentence:', sent)\n",
    "    for token in sent:\n",
    "        print('{:8} {:6} {:4} {}'.format(token.text, token.pos_, token.tag_, token.lemma_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45484e8e",
   "metadata": {},
   "source": [
    "### Syntaxtic analysis and dependency trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "process2 = spacy.load('fr_core_news_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = process2('Le chat boit le lait.')\n",
    "\n",
    "for token in xr:\n",
    "    print('{:8} {:6} {:8} {:6}'.format(token.text, token.pos_, token.lemma_, token.dep_))\n",
    "   \n",
    "\n",
    "spacy.displacy.render(xr, style=\"dep\", jupyter=True)  \n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Your turn to play a bit with syntactic analysis and enter new sentences, possibly complex ones. The example \n",
    "# here is in French but you can obviously do the same with the English pipeline (replace process2 by process).\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dad773",
   "metadata": {},
   "source": [
    "### Word representations and lexical semantics\n",
    "\n",
    "We will play with the two main ways of representing lexical semantics: \n",
    "- word nets, encoding word senses and enconding lexical relations such as homonymy, synonymy, hyperonymy, hyponymy, etc. \n",
    "- distributional semantics as encoded by word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdf2de",
   "metadata": {},
   "source": [
    "#### Playing with word net\n",
    "\n",
    "We we look at what's encoded in wordnet and play a bit with the senses of the token 'rat'. You can get more\n",
    "information on word net from https://wordnet.princeton.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41597690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We start by playing with the senses associated to a token (under the form of sets of synonyms (ala synsets),\n",
    "# then play with one of the senses to see what associated information we can get from the word sense. \n",
    "#\n",
    "\n",
    "print(\"Senses of the token 'rat' are:\")\n",
    "for s in wn.synsets('rat'):\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "\n",
    "print(\"\\nSenses of the noun 'rat' are:\")\n",
    "for s in wn.synsets('rat', pos=wn.NOUN):\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "# now let's focus on the rat as an animal and see what are the relations\n",
    "name = 'rat.n.01'\n",
    "synset = wn.synset(name)\n",
    "\n",
    "# we can list lemmas \n",
    "print(\"\\nLemmas referring to '{}':\".format(name), [str(lemma.name()) for lemma in synset.lemmas()])\n",
    "print(\"Lemmas referring to 'dog.n.01':\", [str(lemma.name()) for lemma in wn.synset('dog.n.01').lemmas()])\n",
    "\n",
    "# we can get lexical relations such as \n",
    "print(\"\\nHypernyms of '{}':\".format(name))\n",
    "for s in synset.hypernyms():\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "print(\"Hyponyms of '{}':\".format(name))\n",
    "for s in synset.hyponyms():\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Try another token and/or synset\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We can use the graph of hypernyms/hyponyms (and a few other ones) to compare two senses\n",
    "#\n",
    "\n",
    "name1 = 'rat.n.01'\n",
    "synset1 = wn.synset(name1)\n",
    "\n",
    "name2 = 'man.n.01'\n",
    "synset2 = wn.synset(name2)\n",
    "\n",
    "lch = synset1.lowest_common_hypernyms(synset2)\n",
    "sim = synset1.wup_similarity(synset2)\n",
    "\n",
    "print(\"Distance between '{}' and '{}' is {:.2f} with lowest common ancestor(s):\".format(name1, name2, sim))\n",
    "for s in lch:\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "    \n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Try a few other pairs, e.g., searching for senses closer to either 'rat.n.01' or 'man.n.01'. What\n",
    "# happens if we take a hyperonym/hyponym? Another sense of the token 'rat'?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d26845",
   "metadata": {},
   "source": [
    "#### Playing with word embeddings\n",
    "\n",
    "Word embedding is a form of distributional semantics, in particular for models that were designed with an \n",
    "explicit distributional hypothesis such as word2vec, glove, fasttext. We will use the word embeddings that are encoded within spaCy's pipeline. \n",
    "\n",
    "spaCy's pipeline comes with a \"vocabulary\", .e.g, the list of tokens that can be processed. We can access to the tokens of the vocabulary and to embedded vectors for the tokens. For the English and French pipelines, these are fastext vectors from Explosion: cf. https://github.com/explosion/spacy-vectors-builder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's start by playing a bit with a few vectors, see what dimension they have and how they can \n",
    "# be used to compute semantic similarity between two tokens.\n",
    "\n",
    "# get access to the pipeline's vocabulary\n",
    "vocab = process.vocab \n",
    "\n",
    "# get three tokens from the vocabulary\n",
    "token1 = vocab['apple']\n",
    "token2 = vocab['orange']\n",
    "token3 = vocab['car']\n",
    "\n",
    "# check that a token has an embedded representation and the dimension of the embedded space\n",
    "if token1.has_vector == True:\n",
    "    print('Embedding size is', len(token1.vector))\n",
    "dim = len(token1.vector)\n",
    "\n",
    "print(\"distance between tokens 'apple' and 'orange' is {:.3f}\".format(token1.similarity(token2)))\n",
    "print(\"distance between tokens 'apple' and 'car' is {:.3f}\".format(token1.similarity(token3)))\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# You can verify that the distance between the tokens is indeed the cosine similarity between the \n",
    "# two corresponding vectors.\n",
    "#\n",
    "# Tips: in numpy (as np), np.norm() is the norm of a vector, np.linalg.dot(v1,v2) provides the dot\n",
    "# product between the two vectors. \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d3a3a",
   "metadata": {},
   "source": [
    "We will now visualize a scatter plot of the vectors for the words that are in simlex. Simlex lists pairs of words along with human judgement of how semantically related (not exactly the same as similar) they are on a scale from 0 (least similar) to 10 (most similar):\n",
    "\n",
    "love sex 6.77 \n",
    "tiger cat 7.35\n",
    "tiger tiger 10\n",
    "book paper 7.46\n",
    "computer keyboard 7.62\n",
    "computer internet 7.58\n",
    "\n",
    "We will load the simlex file, list the words/tokens therein and do a scatter plot of the word embeddings with t-SNE using scitkit learn TSNE() module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://people.irisa.fr/Guillaume.Gravier/teaching/ENSAI/data/simlex-1.csv\n",
    "\n",
    "#\n",
    "# Load simlex data from csv file\n",
    "#\n",
    "import csv\n",
    "\n",
    "simlex = []\n",
    "with open('simlex-1.csv', 'r') as f:\n",
    "    for row in csv.DictReader(f):\n",
    "      simlex.append([row['Word 1'], row['Word 2'], row['Human (mean)']])\n",
    "print('loaded', len(simlex), 'pairs')\n",
    "\n",
    "wordlist = list(set([x[0] for x in simlex] + [x[1] for x in simlex]))\n",
    "nwords = len(wordlist)\n",
    "print('simlex contains', nwords, 'different words/tokens')\n",
    "\n",
    "#\n",
    "# scikit learn TSNE() takes as input either the list of vectors in a nwordsxdim matrix,\n",
    "# or the list of distances (not similarity) between individual points as a nwordsxnwords\n",
    "# symetric matrix. In the first case, one must provide the appropriate distance function\n",
    "# between vectors. We will rather go with the second option taking advantage of the \n",
    "# similarity() function of spaCy. Last, what we need is a matrix with pairwise distances\n",
    "# between words, not similarities! In other words d(x,x)=0, not 1. Cosine similarities are \n",
    "# between -1 (opposite directions) and 1 (same direction), the last case corresponding to\n",
    "# a null distance. We thus defined d(x,y) = 1 - cos(x,y). \n",
    "#\n",
    "X = np.empty((nwords, nwords), dtype='float32')\n",
    "for i in range(nwords):\n",
    "  X[i,:] = [1 - vocab[wordlist[i]].similarity(vocab[x]) for x in wordlist]\n",
    "\n",
    "# dirty hack because some  cosine similarities are slightly below -1 (inperfect arithmetic in computers)\n",
    "c = np.where(X < 0)\n",
    "for i in range(len(c[0])):\n",
    "  X[c[0][i], c[1][i]] = 0.0\n",
    "\n",
    "# run t-SNE 2D projection\n",
    "Y = TSNE(n_components=2, metric='precomputed', init='random', random_state=0, method='exact').fit_transform(X)\n",
    "print('Shape of the projection Y:', Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49654207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: visualize the scatter plot of points in Y, printing the corresponding words. This can be done with\n",
    "# plt.annotate('word', xy=(x_coord,y_cord)) to place the string 'word' at (x_coord,y_cord)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c59bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensai",
   "language": "python",
   "name": "ensai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
