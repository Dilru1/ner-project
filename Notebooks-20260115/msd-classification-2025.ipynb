{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7adbe2",
   "metadata": {},
   "source": [
    "# Document classification notebook\n",
    "\n",
    "This notebook illustrate the lecture on document classification. See first lecture notebook or moodle for explanations on how to set up the proper environment. We will basically here make use of the same environment however augmented with pytorch.\n",
    "\n",
    "## Dataset \n",
    "\n",
    "We will make use of the SST2 dataset from GLUE, a dataset dedicated to sentiment analysis. The task at hand is the classification of short utterances stating an opinion about a movue as expressing a positive (1) or a negative (0) opinion.\n",
    "\n",
    "SST2 data was initially downloaded from https://dl.fbaipublicfiles.com/glue/data/SST-2.zip. Unfortunately doesn't have a label on the test data and there are discrepancies between train and dev, the latter having punctuation marks that the former does not have. We'll thus downsize the train set and split it into train, validation and test subsets. \n",
    "\n",
    "More info on the data at https://nlp.stanford.edu/sentiment/index.html.\n",
    "\n",
    "Note that data preparation with SST2 is minimal as utterances are short, already cleaned with no punctuations and weird signs. \n",
    "\n",
    "If you want to explore further text classification and data preparation, you can also play with the IMDb dataset that contains movie reviews that are much longer than single utterances. See https://ai.stanford.edu/~amaas/data/sentiment for details. \n",
    "\n",
    "## Methods\n",
    "\n",
    "The idea of this notebook is to illustrate a number of methods for text classification. We will guide you through: \n",
    "- an average embedding approach with a neural network classifier\n",
    "- a recurrent neural network approach (in a subsequent lecture)\n",
    "\n",
    "But you can easily implement and compare with the following two approaches:\n",
    "- Naive Bayes classifier (e.g., with sklearn.naive_bayes.MultinomialNB)\n",
    "- tf-idf vector space model with a k-nearest neighbor classifier (see illustration notebook on moodle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b3ea4",
   "metadata": {},
   "source": [
    "## Loading and tokenizing the dataset\n",
    "\n",
    "Let's first load the data, clean a bit and tokenize. Last cell provides basic statistics on the corpus.\n",
    "\n",
    "Punctuation marks were already removed in SST2 and all data is uncased so tokenization remains minimal here. It basically consists in separating tokens based on spaces after getting rid of weird symbols we don't want to bother with.\n",
    "\n",
    "Note that in real life, if data hasn't been pre-processed as it is now, you'd have to invoke a real tokenizer such as the ones we made us of during the first lecture (e.g., NLTK sent_tokenize and word_tokenize, one of spaCy's pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's first load raw data from the CSV files. \n",
    "#\n",
    "\n",
    "import csv\n",
    "\n",
    "def load_tsv_data(fn: str) -> list[dict]:\n",
    "    '''\n",
    "    Load data from file\n",
    "    '''\n",
    "    \n",
    "    with open(fn, 'r') as f:\n",
    "        dat = [x for x in csv.DictReader(f, delimiter=\"\\t\")] \n",
    "\n",
    "    return dat    \n",
    "    \n",
    "data = load_tsv_data('./data.NOSAVE/sst2-train.tsv')\n",
    "\n",
    "for i in range(10):\n",
    "    print('sample {} -- class={}, string=/{}/'.format(i, data[i]['label'], data[i]['sentence']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Basic tokenization and cleansing of the utterances to classify. \n",
    "# \n",
    "\n",
    "def clean_and_tokenize_utterance(x: dict) -> dict:\n",
    "    '''\n",
    "    Tokenize utterance with basic rules and convert labels to int. Input an entry of the dataset as\n",
    "    a dict() and returns the dictionary augmented with the list of tokens (as strings). Also converts\n",
    "    the label to an integer.\n",
    "    '''\n",
    "    unwanted =  (\"``\", \"''\", \"'\", \"`\", \"--\", \",\", \".\")\n",
    "    \n",
    "    x['tokens'] = [token for token in x['sentence'].split() if token not in unwanted]\n",
    "    x['label'] = int(x['label'])\n",
    "    \n",
    "    return x\n",
    "\n",
    "# apply the utterance-level tokenizer to each data item\n",
    "data = list(map(clean_and_tokenize_utterance, data))\n",
    "\n",
    "for i in range(10):\n",
    "    print('sample {} -- class={}, string=/{}/, tokens={}'.format(i, data[i]['label'], data[i]['sentence'], data[i]['tokens']))\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Question: What do you think of the tokenization? Is it good, bad? What could cause problems later on \n",
    "# in the process of designing an utterance classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Check some basic statistics on the data\n",
    "#\n",
    "\n",
    "nsamples = len(data)\n",
    "nlabels = 2\n",
    "nlabels0 = len([x for x in data if x['label'] == 0])\n",
    "nlabels1 = len([x for x in data if x['label'] == 1])\n",
    "\n",
    "ntokens = [len(x['tokens']) for x in data]\n",
    "\n",
    "m = statistics.mean(ntokens)\n",
    "m2 = statistics.median(ntokens)\n",
    "sdev = statistics.stdev(ntokens)\n",
    "    \n",
    "print('{:35s} {}  {}/{}'.format('Number of samples in dataset', nsamples, nlabels0, nlabels1))\n",
    "print('{:35s} min={}  max={}  mean={:.1f}  median={}  sdev={:.1f}'.format('Number of tokens per utterance', min(ntokens), max(ntokens), m, m2, sdev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825d1d8",
   "metadata": {},
   "source": [
    "## Building the vocabulary\n",
    "\n",
    "As we will design models that define statistics on tokens, we first have to build the vocabulary that we will be able to represent. A common choice when not using subword tokenization is to consider the most frequent tokens as the vocabulary.\n",
    "\n",
    "We'll limit ourselves to simple terms (as opposed to complex terms such as 'can opener' or 'neural network') and simply select the most frequent terms in the corpus.\n",
    "\n",
    "Note that many toolkits for NLP provide a sort of equivalent function, e.g., \n",
    "- gensim.corpora.dictionary.Dictionary -- https://radimrehurek.com/gensim/corpora/dictionary.html#\n",
    "- tf.keras.preprocessing.text.Tokenizer -- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "- torchtext.vocab -- https://pytorch.org/text/stable/vocab.html\n",
    "\n",
    "But for pedagogical purposes, we'll do it once all by ourselves ;).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1641eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Check all tokens that appear in the dataset and count the number of times they appear.\n",
    "#\n",
    "\n",
    "counter = Counter()\n",
    "   \n",
    "for item in data:\n",
    "    #\n",
    "    # If we were to implement filters, e.g., on the POS tags, that's the place\n",
    "    # where it could/should be done. We'd for instance create a temporary list\n",
    "    # of relevant tokens before updating counter instead of taking all the tokens\n",
    "    # as is.\n",
    "    #\n",
    "    counter.update(item['tokens'])\n",
    "\n",
    "counter = dict(sorted(counter.items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "#\n",
    "# Pretty print a number of things\n",
    "#\n",
    "print('total number of tokens in dataset =', len(counter))\n",
    "print('most frequent tokens:')\n",
    "for x in list(counter.keys())[:20]:\n",
    "    print(f\"   {x:18}  {counter[x]}\")\n",
    "print('\\nleast frequent tokens:')\n",
    "for x in list(counter.keys())[-20:]:\n",
    "    print(f\"   {x:18}  {counter[x]}\")\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Comment the most and least frequent tokens and think about how you could get\n",
    "# a list of tokens of interest other than by selecting the most frequent ones.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's build the vocabulary from there, selecting the most frequent terms. Note that another common\n",
    "# choice is to keep tokens that appear at least a certain number of times in the dataset.\n",
    "#\n",
    "# We will include two special tokens in the vocabulary:\n",
    "#.  - PAD: the id of the padding token required for sequence models\n",
    "#   - UNK: the id to assign to tokens that are not in the vocabulary\n",
    "#\n",
    "# The vocabulary will be dictionary mapping string to ids.\n",
    "#\n",
    "\n",
    "nterms = 4000 # number of terms to keep in the vocabulary\n",
    "\n",
    "vocab = {'PAD': 0, 'UNK': 1} # initialize with the two special tokens before updating with actual regular tokens\n",
    "offset = len(vocab)\n",
    "vocab.update({x: i+offset for i,x in enumerate(list(counter.keys())[:nterms])})\n",
    "\n",
    "print(len(vocab))\n",
    "print(list(vocab.keys())[:20])\n",
    "\n",
    "# for pretty printing later on, let's build a reverse mapping id2str from token IDs to the corresponding string\n",
    "id2str = list(vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf51b26",
   "metadata": {},
   "source": [
    "## Encoding and splitting data to create pytorch datasets and dataloaders\n",
    "\n",
    "Last step before we can start building classifier, we have to split the dataset into train/validation/test and encode input sequences into list of integers rather than list of strings. \n",
    "\n",
    "We also have to convert each dataset into a pytorch Dataset that will be batched automatically through the Dataloader. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b1e38",
   "metadata": {},
   "source": [
    "### Encoding and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18abd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Encoding the data simply means converting tokens from string to integer ids that are more suited for\n",
    "# modeling and embedding. We will simply add a field 'ids' to each data item holding the list of token ids\n",
    "#\n",
    "\n",
    "def encode_utterance(x: dict) -> dict:\n",
    "    '''\n",
    "    Encode utterance according to the mapping provided by v. Input an entry of the dataset as\n",
    "    a dict() and returns the dictionary augmented with the list of tokens (as strings). For \n",
    "    practical reasons, we're also adding a ids_no_unk field where unknown token ids are discarded.\n",
    "    '''\n",
    "    global vocab\n",
    "    \n",
    "    unk_id = vocab['UNK']\n",
    "    \n",
    "    x['ids'] = [vocab.get(token, unk_id) for token in x['tokens']] \n",
    "    x['ids_no_unk'] = [x for x in x['ids'] if x != unk_id]\n",
    "    \n",
    "    return x\n",
    "\n",
    "data = list(map(encode_utterance, data))\n",
    "\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's finally split the data into three datasets\n",
    "#\n",
    "\n",
    "fold = dict()\n",
    "\n",
    "fold['train'], buf = train_test_split(data, test_size=0.3, random_state=42)\n",
    "fold['valid'], fold['test'] = train_test_split(buf, test_size=0.5, random_state=42)\n",
    "\n",
    "for s in ('train', 'valid', 'test'):\n",
    "    buf = fold[s]\n",
    "    n0 = len([x for x in buf if x['label'] == 0])\n",
    "    n1 = len([x for x in buf if x['label'] == 1])\n",
    "\n",
    "    print('{:6s} = {}  {}/{}'.format(s, len(buf), n0, n1))\n",
    "\n",
    "print(fold['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3405365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save dataset folds for later reuse\n",
    "#\n",
    "\n",
    "with gzip.open('data.NOSAVE/sst2-tokenized-folds.json.gz', 'wb') as f:\n",
    "    json.dump(fold, f)\n",
    "    \n",
    "#\n",
    "# reload\n",
    "#\n",
    "# with gzip.open('data.NOSAVE/sst2-tokenized-folds.json.gz', 'rb') as f:\n",
    "#     fold = json.load(f)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c60bc",
   "metadata": {},
   "source": [
    "### Turn datasets into a pytorch Dataset\n",
    "\n",
    "Batching requires that all sequences have the same length, at least in a batch. \n",
    "\n",
    "In practice, this is ensured with a collator function that 0-pads the sequences on a per batch basis. To make it easier and more explicit, we will force all our sequences in the dataset to have the same length right from the start and don't bother with a collator. But we have to keep in mind this is suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the dataset class to hold the labels and the (padded) input ids referred to as 'encodings'.\n",
    "#\n",
    "# I chose to tokenize, encode and pad outside the dataset but this could all be embedded\n",
    "# in the class constructor. See, e.g.,\n",
    "# as in https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb#scrollTo=3vWRDemOGxJD\n",
    "#\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        \n",
    "        assert(len(encodings) == len(labels))\n",
    "        \n",
    "        self.nsamples = len(labels)\n",
    "        \n",
    "        # print(f'Initializing dataset with {self.nsamples} entries')\n",
    "        \n",
    "        self.encodings = encodings # list[list[int]]: contains the padded list of token ids for each sample\n",
    "        self.labels = labels # list[int]: contains the label for each sample\n",
    "        self.nlabels = len(set(labels)) # int: number of labels in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns a dictionary containing the label and padded token ids for a sample\n",
    "        '''\n",
    "        \n",
    "        # print(f'accessing dataset item at index {idx}')\n",
    "        # print(torch.tensor(self.encodings[idx]), torch.tensor(self.labels[idx]))\n",
    "\n",
    "        return {'ids': torch.tensor(self.encodings[idx]), 'label': torch.tensor(self.labels[idx])}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "#\n",
    "# Define basic padding function\n",
    "#\n",
    "def pad_utterances(x: list, maxlen: int, pad_id: int = 0) -> list:\n",
    "    '''\n",
    "    Pad all input utterances up to maxlen, truncating if need be.\n",
    "    \n",
    "    Returns a list of padded ids. \n",
    "    '''\n",
    "    \n",
    "    return [(ids + [pad_id] * (maxlen - len(ids)))[:maxlen] for ids in x]\n",
    "\n",
    "\n",
    "#\n",
    "# Function to convert the dataset as a list[dict] into a proper torch.Dataset object\n",
    "# \n",
    "def to_dataset(_data: list[dict], key: str = 'ids', maxlen: int = -1, pad_id: int = 0) -> MyDataset:\n",
    "    '''\n",
    "    Convert data as processed before into a proper pyTorch dataset with the specified tokenizer. \n",
    "    If maxlen <= 0, then we take the maximum length within the sequence.\n",
    "    '''\n",
    "\n",
    "    if maxlen <= 0:\n",
    "        maxlen = max([len(x[key]) for x in _data])\n",
    "        print('maxlen set to', maxlen)\n",
    "        \n",
    "    ids = [x[key] for x in _data]\n",
    "    labels = [x['label'] for x in _data]\n",
    "    encodings = pad_utterances(ids, maxlen)\n",
    "    \n",
    "    return MyDataset(encodings, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dict()\n",
    "\n",
    "ds['train'] = to_dataset(fold['train'], key='ids_no_unk', maxlen=10)\n",
    "ds['valid'] = to_dataset(fold['valid'], key='ids_no_unk', maxlen=10)\n",
    "ds['test'] = to_dataset(fold['test'], key='ids_no_unk', maxlen=10)\n",
    "\n",
    "print('Training set:  nsamples =', ds['train'].nsamples, ' nlabels =', ds['train'].nlabels)\n",
    "\n",
    "for i in range(3):\n",
    "    print(fold['train'][i])\n",
    "    print('   >>', ds['train'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde0e09",
   "metadata": {},
   "source": [
    "## A basic (toy) text classification neural network\n",
    "\n",
    "Let's start text classification machinery with the basic model that we saw in the lecture that embeds the tokens, takes the average embedding and run a basic feed-forward classifier on top of it.\n",
    "\n",
    "This section covers several steps in (pytorch) model design\n",
    "1. define the model, i.e., its architecture\n",
    "2. organize the dataset into batches, achieved with a Dataloader object in pytorch\n",
    "3. define the training setup, i.e., optimizer and loss function along with a few parameters (e.g., number of epochs, learning rate)\n",
    "4. run the training loop\n",
    "5. evaluate results on the test set\n",
    "\n",
    "In pytorch, the training loop has to be written explicitly (contrary to the fit() method in scikit-learn or tensorflow). Do does the evaluation loop (contrary to evaluate()). To facilitate things, two generic functions are given, namely train_step() and eval_step(): see below for more comments. These two functions are defined here since it's the first model we are conceiving but they will be used as is for the subsequent models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6bf62",
   "metadata": {},
   "source": [
    "### Creating the model and batches\n",
    "\n",
    "Let's first create the model, batch the data via a DataLoader (this is where we're happy that all encoded sequences are of the same length) and illustrate how we can pass on data for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ecad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the model as a torch.nn.Module\n",
    "#\n",
    "# Note that the softmax is not necessary when training with CrossEntropyLoss() which will\n",
    "# apply the softmax to the logis. It is however required if training with NLLLoss().\n",
    "\n",
    "class AvgEmbeddingNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocsize, nclasses = 2, embed_dim = 200, dropout = None):\n",
    "        super(AvgEmbeddingNN, self).__init__()\n",
    "\n",
    "        self.nclasses = nclasses\n",
    "        self.vocabulary_size = vocsize\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocsize, embed_dim, padding_idx = 0)\n",
    "        self.dropout = torch.nn.Dropout(dropout) if dropout != None else None\n",
    "        self.linear = torch.nn.Linear(embed_dim, nclasses)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        x = self.embedding(kwargs['ids']) # embed input ids -- batch_size * maxlen * embed_dim\n",
    "        if self.dropout != None: # dropout embeddings\n",
    "            x = self.dropout(x)        \n",
    "        x = torch.mean(x, dim=1) # average embeddings yielding an average tensor -- batch_size * embed_dim\n",
    "        x = self.softmax(self.linear(x)) # project into posterior probabilities -- batch_size * nclasses\n",
    "\n",
    "        return x\n",
    "        \n",
    "model1 = AvgEmbeddingNN(len(vocab), nclasses = ds['train'].nlabels, embed_dim = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create batched dataset with data loaders\n",
    "#\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "loader = dict()\n",
    "loader['train'] = DataLoader(ds['train'], batch_size=batch_size, shuffle=True) # set to False for debugging purposes\n",
    "loader['valid'] = DataLoader(ds['valid'], batch_size=batch_size)\n",
    "loader['test'] = DataLoader(ds['test'], batch_size=batch_size)\n",
    "\n",
    "print('Number of samples:', len(ds['train']))\n",
    "print(f'Number of training batches:', len(loader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for illustration purposes only, showing how batches cab be passed and processed to\n",
    "# the model's forward method at iunference time.\n",
    "\n",
    "batch = next(iter(loader['train']))\n",
    "print(batch['ids'].shape)\n",
    "print(batch)\n",
    "\n",
    "# Option 1: passing the ids tensor directly as a named argument\n",
    "output = model1(ids=batch['ids'])\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n",
    "# Option 2: passing an arbitrary number of named arguments, of which only ids=ids will be used\n",
    "output = model1(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d8b50",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "In pytorch, we have to explicitly write the training iterations. The code will vary little from one model to the other but still, we have to write the training loop explicitly. To simplify things, you are provided with two key functions defined in the following cells. The actual training and evaluation of the model follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2076f",
   "metadata": {},
   "source": [
    "#### Defining the basic building blocks of the training loop\n",
    "\n",
    "Two key functions are provided here:\n",
    "\n",
    "- train_step(): run one epoch given a model, a data loader, a loss function and an optimizer function; the function that should work for all models we'll be playing with\n",
    "\n",
    "- eval_step(): evaluate the model's accuracy given a model, a data loader\n",
    "\n",
    "We finally instantiate in the last cell of this section the various functions that we'll be needing for the training such as the optimizer, the loss function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(_model, _loader, _loss, _optim, device=\"cpu\", report=0):\n",
    "    '''\n",
    "    Generic training step.\n",
    "\n",
    "    Assumes loader returns batches where the labels are accessed with the 'label' keyword.\n",
    "    All other keywords are passed as **kwargs to the model.\n",
    "    \n",
    "    If report is set to a number, reports stats on training every 'report' batches.\n",
    "\n",
    "    :return: total_loss accumulated throughout the epoch\n",
    "    '''\n",
    "\n",
    "    _model.train(True)\n",
    "    total_loss = 0.\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, batch in enumerate(_loader):\n",
    "        _optim.zero_grad()\n",
    "\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "        outputs = _model(**inputs)\n",
    "\n",
    "        loss = _loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        _optim.step()\n",
    "\n",
    "        if report != 0 and i % report == report - 1:\n",
    "            print('  batch {} avg. loss per batch={:.4f}'.format(i + 1, running_loss / report))\n",
    "            running_loss = 0.\n",
    "\n",
    "    _model.train(False)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(_model, _loader, device='cpu', loss_fn=None):\n",
    "    '''\n",
    "    Evaluate the model's performance on data within loader.\n",
    "    \n",
    "    :return: \n",
    "    total_loss accumulated throughout the batches\n",
    "    accuracy\n",
    "    '''\n",
    "    \n",
    "    # logger.debug('running eval_acc_recprec() on %s', device)\n",
    "\n",
    "    _model.eval()  # disable training mode\n",
    "\n",
    "    posteriors = torch.empty((0, _model.nclasses)).to(device)\n",
    "    labels = torch.empty((0)).to(device)\n",
    "\n",
    "    total_loss = 0.\n",
    "\n",
    "    for batch in _loader:\n",
    "        batch_labels = batch['label'].to(device)\n",
    "        labels = torch.cat((labels, batch_labels), dim=0)\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = _model(**inputs)\n",
    "\n",
    "        posteriors = torch.cat((posteriors, outputs), dim=0)\n",
    "\n",
    "        if loss_fn != None:\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "    labels = labels.cpu()\n",
    "    posteriors = posteriors.cpu()\n",
    "    predictions = torch.argmax(posteriors, dim=-1)\n",
    "    \n",
    "    return total_loss, accuracy_score(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff079d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Last but not least, we have to set the training parameters and instatiate all the objects \n",
    "# needed for training.\n",
    "#\n",
    "\n",
    "lr = 1e-4\n",
    "nepochs = 10\n",
    "report_freq = 200\n",
    "\n",
    "# check what device we can work on\n",
    "if torch.backends.mps.is_built(): # MPS GPU library for MacOS -- requires metal to be installed\n",
    "    device = \"mps\"\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available(): # CUDA GPU acceleration available\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f'Running on {device} device')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=lr)\n",
    "celoss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# At last, here we go with the main training loop, iterating over epochs\n",
    "#\n",
    "\n",
    "model1.to(device)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    total_loss = train_step(model1, loader['train'], celoss, optimizer, device=device, report=report_freq)\n",
    "    _, trn_acc = eval_step(model1, loader['train'], device=device, loss_fn=None)\n",
    "    \n",
    "    val_loss, val_acc = eval_step(model1, loader['valid'], device=device, loss_fn=celoss)\n",
    "\n",
    "    print('  **train** avg_loss={:.4f}    acuracy={:.2f}%'.format(total_loss / len(loader['train']), 100 * trn_acc))\n",
    "    print('  **valid** avg_loss={:.4f}    acuracy={:.2f}%'.format(val_loss / len(loader['valid']), 100 * val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tst_acc = eval_step(model1, loader['test'], device=device, loss_fn=None)\n",
    "\n",
    "print('  **test** acuracy={:.2f}%'.format(100 * tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3098d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "#\n",
    "# Visualize the word embeddings learned by the neural network with tSNE. Do we observe the \n",
    "# distributional semantic properties?\n",
    "#\n",
    "# Revisit the code above to work on lemmas rather than tokens, possibly limiting yourself to noun,\n",
    "# verbs and adjectives which are the most relevant POS tags for polarity detection (adjectives alone \n",
    "# are already pretty good)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63dc8d0",
   "metadata": {},
   "source": [
    "## Going further with the notebook\n",
    "\n",
    "We will get back to this task after the lecture on recurrent neural networks and with large language models. \n",
    "\n",
    "In the meantime, you can try to improve by a better definition of the vocabulary. The choice of vocabulary is rather poor here and deserves significant improvement. For polarity detection, there's no need to work with \"words\" and lemmas are sufficient. Also for the approach based on average embeddings, function words (determiners, auxiliary, etc,) are not necessary and can be safely removed. You should adapt this first method so that the vocabulary is built from lemmatized input keeping only nouns, verbs and adjectives in the vocabulary. To do so, you need to run POS tagging and lemmatization (see first notebook on how to do that with NLTK for instance), adapt the vocabulary and hence the subsequent steps (preparation and encoding of the data).\n",
    "\n",
    "You can experiment also with discarding / not discarding the UNK tokens and measure what impact it has.\n",
    "\n",
    "Also, if you might want to compare with basic statistical methods based on symbolic word representations rather than embedding, for instance implementing a vector space model with tf-idf weights along with a k-nn classifier or a naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf4e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
