{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061c32ab",
   "metadata": {},
   "source": [
    "# Fine-tuning a LLM\n",
    "\n",
    "This notebook illustrates how to finetune a LLM for document classification. We will make use of a bidirectional transformer pre-trained as a masked-language model (MLM), BERT-style, and insert that into an architecture for document classification, adding a feed-forward layer (aka classification head) on top of the embedding of the [CLS] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a57023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc6a4a",
   "metadata": {},
   "source": [
    "## Load a BERT-like model\n",
    "\n",
    "We'll choose a small (distilled) model to make things fast and easy but any bidirectional (or even causal) model can be used here. We'll go for https://huggingface.co/distilbert/distilbert-base-uncased but you might want to try other options such as https://huggingface.co/distilbert/distilroberta-base. \n",
    "\n",
    "Note that these models were pre-trained as masked language models on English data and are therefore only suited for English texts. If you are to process French texts, typical models are CamemBERT and FlauBERT, some of them having distilled (smaller) versions. \n",
    "\n",
    "We will simply load the encoder (AutoModel), without the language model classification (AutoModelForMaskedLM) that we used in the previous notebook. Hence, the output has no logits but only embeddings.\n",
    "\n",
    "We also briefly illustrate how to tokenize a list of texts at once, also taking care of padding and truncation (the tokenizer does that for you in a pretty efficient and nice manner, so let's simply make use of this feature). See https://huggingface.co/docs/transformers/main/en/model_doc/distilbert for details on the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load tokenizer and model\n",
    "#\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased' # distilroberta-base\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) # load tokenizer\n",
    "bert = AutoModel.from_pretrained(checkpoint) # load model\n",
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4531e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is a reminder of how the model can be used. Contrary to the previous notebook, we illustrate\n",
    "# how multiple texts can be passed on to the tokenizer and the model. Note that the former directly \n",
    "# takes care of padding and truncation.\n",
    "#\n",
    "\n",
    "texts = ['I enjoy playing with LLMs.', 'Finetuning is fun!']\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print('Output of tokenizer:\\n', inputs)\n",
    "print('Tokens and text:', tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]), '--', tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert(**inputs)\n",
    "\n",
    "print('Output shape:', outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499f648",
   "metadata": {},
   "source": [
    "## Get data for document classification\n",
    "\n",
    "We will use the same SST2 dataset as we already used for document classification. \n",
    "\n",
    "### Read the data and split into train/valid/test folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize_utterance(x: dict) -> dict:\n",
    "    '''\n",
    "    Tokenize utterance with basic rules and convert labels to int. Input an entry of the dataset as\n",
    "    a dict() and returns the dictionary augmented with the list of tokens (as strings). Also converts\n",
    "    the label to an integer.\n",
    "    '''\n",
    "    unwanted =  (\"``\", \"''\", \"'\", \"`\", \"--\", \",\", \".\")\n",
    "    \n",
    "    x['tokens'] = [token for token in x['sentence'].split() if token not in unwanted]\n",
    "    x['label'] = int(x['label'])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's first load raw data from the CSV files. \n",
    "#\n",
    "\n",
    "fn = './data.NOSAVE/sst2-train.tsv'\n",
    "\n",
    "with open(fn, 'r') as f:\n",
    "    data = [x for x in csv.DictReader(f, delimiter=\"\\t\")] \n",
    "\n",
    "data = list(map(clean_and_tokenize_utterance, data))\n",
    "\n",
    "for i in range(10):\n",
    "    print('sample {} -- class={}, string=/{}/'.format(i, data[i]['label'], data[i]['sentence']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# You can adjust here to get more or less data. Commented lines make use of the whole dataset while\n",
    "# actual code only takes a small fraction of the data to make training faster.\n",
    "#\n",
    "\n",
    "fold = dict()\n",
    "\n",
    "# fold['train'], buf = train_test_split(data, test_size=0.3, random_state=42)\n",
    "# fold['valid'], fold['test'] = train_test_split(buf, test_size=0.5, random_state=42)\n",
    "\n",
    "fold['train'], buf = train_test_split(data, test_size=0.8, random_state=42)\n",
    "fold['valid'], buf = train_test_split(buf, test_size=0.9, random_state=42)\n",
    "fold['test'], _ = train_test_split(buf, test_size=0.9, random_state=42)\n",
    "\n",
    "for s in ('train', 'valid', 'test'):\n",
    "    buf = fold[s]\n",
    "    n0 = len([x for x in buf if int(x['label']) == 0])\n",
    "    n1 = len([x for x in buf if int(x['label']) == 1])\n",
    "\n",
    "    print('{:6s} = {}  {}/{}'.format(s, len(buf), n0, n1))\n",
    "\n",
    "for item in fold['train'][:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c6f0b",
   "metadata": {},
   "source": [
    "### Convert data to Dataset\n",
    "\n",
    "The dataset simply stores the encodings as returned by the tokenizer and the corresponding labels. \n",
    "\n",
    "We assume here that input to the tokenizer ia already split into tokens (i.e., input as a list of tokens (as str) rather than a single string to break into tokens -- this is why we have is_split_into_words=True). We also assume that the tokenizer takes care of the padding and truncation.\n",
    "\n",
    "Note that datasets can be saved if you don't want to run these steps over and over again. Here's how to do that:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "fn = \"pytorch-datasets.pkl\"\n",
    "\n",
    "with open(fn, \"wb\") as f:\n",
    "    pickle.dump(ds, f)\n",
    "\n",
    "with open(fn, \"rb\") as f: \n",
    "    ds = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e425e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dataset class definition and convertion function from pre-processed data to torch Dataset\n",
    "#\n",
    "\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, _data, _tokenizer):\n",
    "        self.encodings = _tokenizer([x['tokens'] for x in _data])\n",
    "        self.labels = [x['label'] for x in _data]\n",
    "        self.nlabels = len(set(self.labels))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "tok_fn =  lambda x: tokenizer(x, is_split_into_words=True, truncation=True, padding=True)\n",
    "\n",
    "ds = dict()\n",
    "ds['train'] = SST2Dataset(fold['train'], tok_fn)\n",
    "ds['valid'] = SST2Dataset(fold['valid'], tok_fn)\n",
    "ds['test'] = SST2Dataset(fold['test'], tok_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fold['train'][0]['tokens'])\n",
    "print(ds['train'][0])\n",
    "print(tokenizer.decode(ds['train'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600095e",
   "metadata": {},
   "source": [
    "## Create and train a classification model\n",
    "\n",
    "We will define a simple model that simply takes the embeddig of the [CLS] token as provided by the BERT-like encoder and run it through a feed-forward layer to predict logits for the output classes. We will thus use a standard cross-entropy loss function, following exactly the same training steps as what we did in the document classification notebook. We will limit ourselves to one or two iterations as training can be somewhat slow with transformers.\n",
    "\n",
    "We can directly make use of the train_step() and eval_step() functions that we defined previously. Also remember you cna save models to disk with\n",
    "\n",
    "```python\n",
    "fn = 'model.pt'\n",
    "torch.save(model.state_dict(), fn)\n",
    "model.load_state_dict(torch.load(fn, weights_only=True))\n",
    "```\n",
    "\n",
    "**Important:** Transormers are known to require a smaller learning rate than basic of recurrent neural networks. We wil thus lower the learning rate to 1e-5 (as opposed to 1e-4 or 5e-4 typically used with standard networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the model to do the following:\n",
    "#    1. encode input with the BERT encoder\n",
    "#    2. get embedding of token [CLS] (dimension is 768, i.e.,  encoder.config.dim)\n",
    "#    3. run classification from the [CLS] embedding through two feed-forward layers\n",
    "#\n",
    "\n",
    "class BERTClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, _encoder, nclasses = 2, dropout = None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "\n",
    "        self.nclasses = nclasses\n",
    "        self.encoder = copy.deepcopy(_encoder) # to avoid modifying the encoder directly\n",
    "        self.dropout = torch.nn.Dropout(dropout) if dropout != None else None\n",
    "        self.linear1 = torch.nn.Linear(self.encoder.config.dim, 100)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(100, nclasses)\n",
    "        # self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        x = self.encoder(**kwargs)        # run batch through the BERT encoder\n",
    "        x = x.last_hidden_state[:,0,:]    # get embedding of [CLS] token for each input in the batch\n",
    "        if self.dropout != None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear1(x)               # project to a 100-d hidden layer\n",
    "        x = self.activation(x)\n",
    "        if self.dropout != None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear2(x)               # project to logits, one per class\n",
    "        # x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = BERTClassifier(bert, nclasses = ds['train'].nlabels, dropout = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f18570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the training and evaluation steps\n",
    "#\n",
    "\n",
    "def train_step(_model, _loader, _loss, _optim, device=\"cpu\", report=0):\n",
    "    '''\n",
    "    Generic training step.\n",
    "\n",
    "    Assumes loader returns batches where the labels are accessed with the 'label' keyword.\n",
    "    All other keywords are passed as **kwargs to the model.\n",
    "    \n",
    "    If report is set to a number, reports stats on training every 'report' batches.\n",
    "\n",
    "    :return: total_loss accumulated throughout the epoch\n",
    "    '''\n",
    "\n",
    "    _model.train(True)\n",
    "    total_loss = 0.\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, batch in enumerate(_loader):\n",
    "        _optim.zero_grad()\n",
    "\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "        outputs = _model(**inputs)\n",
    "\n",
    "        loss = _loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        _optim.step()\n",
    "\n",
    "        if report != 0 and i % report == report - 1:\n",
    "            print('  batch {} avg. loss per batch={:.4f}'.format(i + 1, running_loss / report))\n",
    "            running_loss = 0.\n",
    "\n",
    "    _model.train(False)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def eval_step(_model, _loader, device='cpu', loss_fn=None):\n",
    "    '''\n",
    "    Evaluate the model's performance on data within loader.\n",
    "    \n",
    "    :return: \n",
    "    total_loss accumulated throughout the batches\n",
    "    accuracy\n",
    "    '''\n",
    "\n",
    "    _model.eval()  # disable training mode\n",
    "\n",
    "    logits = torch.empty((0, _model.nclasses)).to(device)\n",
    "    labels = torch.empty((0)).to(device)\n",
    "\n",
    "    total_loss = 0.\n",
    "\n",
    "    for batch in _loader:\n",
    "        batch_labels = batch['label'].to(device)\n",
    "        labels = torch.cat((labels, batch_labels), dim=0)\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = _model(**inputs)\n",
    "\n",
    "        logits = torch.cat((logits, outputs), dim=0)\n",
    "\n",
    "        if loss_fn != None:\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "    labels = labels.cpu()\n",
    "    logits = logits.cpu()\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return total_loss, accuracy_score(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed07ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set training parameters\n",
    "#\n",
    "\n",
    "batch_size = 16\n",
    "lr = 1e-5\n",
    "nepochs = 2\n",
    "report_freq = 100\n",
    "\n",
    "if torch.backends.mps.is_built(): # MPS GPU library for MacOS -- requires metal to be installed\n",
    "    device = \"mps\"\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available(): # CUDA GPU acceleration available\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f'Running on {device} device')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "celoss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Batch data for training\n",
    "#\n",
    "loader = dict()\n",
    "loader['train'] = DataLoader(ds['train'], batch_size=batch_size, shuffle=True) # set to False for debugging purposes\n",
    "loader['valid'] = DataLoader(ds['valid'], batch_size=batch_size)\n",
    "loader['test'] = DataLoader(ds['test'], batch_size=batch_size)\n",
    "\n",
    "print('Number of samples:', len(ds['train']))\n",
    "print(f'Number of training batches:', len(loader['train']))\n",
    "\n",
    "#\n",
    "# Run the training loop\n",
    "#\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    total_loss = train_step(model, loader['train'], celoss, optimizer, device=device, report=report_freq)\n",
    "    _, trn_acc = eval_step(model, loader['train'], device=device, loss_fn=None)\n",
    "    \n",
    "    val_loss, val_acc = eval_step(model, loader['valid'], device=device, loss_fn=celoss)\n",
    "\n",
    "    print('  **train** avg_loss={:.4f}    acuracy={:.2f}%'.format(total_loss / len(loader['train']), 100 * trn_acc))\n",
    "    print('  **valid** avg_loss={:.4f}    acuracy={:.2f}%'.format(val_loss / len(loader['valid']), 100 * val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tst_acc = eval_step(model, loader['test'], device=device, loss_fn=None)\n",
    "\n",
    "print('  **test** acuracy={:.2f}%'.format(100 * tst_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae21e8",
   "metadata": {},
   "source": [
    "## Visualize document embeddings\n",
    "\n",
    "The BERT encoder acts a document embedder, turning a document into a 768-dimensional representation through the layers of transformer-blocks. We can thus visualize these embeddings with t-SNE, before and after fine-tuning the BERT encoder to figure out what exactly fine-tuning does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(_encoder, _loader, nbatches = 10):\n",
    "    '''\n",
    "    Return the document embeddings.\n",
    "    '''\n",
    "    \n",
    "    global batch_size\n",
    "    \n",
    "    nsamples = nbatches * batch_size\n",
    "\n",
    "    embeddings = np.empty((nsamples, bert.config.dim))\n",
    "    labels = np.empty(nsamples)\n",
    "\n",
    "    for i, batch in enumerate(_loader):\n",
    "    \n",
    "        if i == nbatches: break\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v for k, v in batch.items() if k != 'label'}\n",
    "            outputs = _encoder(**inputs)\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            embeddings[i*batch_size + j,:] = outputs.last_hidden_state[j,0,:].detach().numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4eebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: retrieve document embeddings for test documents using (a) the original BERT encoder (before finetuning)\n",
    "# and (b) the adapted encoder within the classification model and plot with t-SNE the corresponding document \n",
    "# embeddings.  \n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
